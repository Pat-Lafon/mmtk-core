<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="rustdoc"><meta name="description" content="Source of the Rust file `src/policy/marksweepspace/malloc_ms/global.rs`."><meta name="keywords" content="rust, rustlang, rust-lang"><title>global.rs - source</title><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../SourceSerif4-Regular.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../FiraSans-Regular.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../FiraSans-Medium.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../SourceCodePro-Regular.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../SourceSerif4-Bold.ttf.woff2"><link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../SourceCodePro-Semibold.ttf.woff2"><link rel="stylesheet" href="../../../../../normalize.css"><link rel="stylesheet" href="../../../../../rustdoc.css" id="mainThemeStyle"><link rel="stylesheet" href="../../../../../ayu.css" disabled><link rel="stylesheet" href="../../../../../dark.css" disabled><link rel="stylesheet" href="../../../../../light.css" id="themeStyle"><script id="default-settings" ></script><script src="../../../../../storage.js"></script><script defer src="../../../../../source-script.js"></script><script defer src="../../../../../source-files.js"></script><script defer src="../../../../../main.js"></script><noscript><link rel="stylesheet" href="../../../../../noscript.css"></noscript><link rel="alternate icon" type="image/png" href="../../../../../favicon-16x16.png"><link rel="alternate icon" type="image/png" href="../../../../../favicon-32x32.png"><link rel="icon" type="image/svg+xml" href="../../../../../favicon.svg"></head><body class="rustdoc source"><!--[if lte IE 11]><div class="warning">This old browser is unsupported and will most likely display funky things.</div><![endif]--><nav class="sidebar"></nav><main><div class="width-limiter"><nav class="sub"><a class="sub-logo-container" href="../../../../../mmtk/index.html"><img class="rust-logo" src="../../../../../rust-logo.svg" alt="logo"></a><form class="search-form"><span></span><input class="search-input" name="search" autocomplete="off" spellcheck="false" placeholder="Click or press ‘S’ to search, ‘?’ for more options…" type="search"><div id="help-button" title="help" tabindex="-1"><a href="../../../../../help.html">?</a></div><div id="settings-menu" tabindex="-1"><a href="../../../../../settings.html" title="settings"><img width="22" height="22" alt="Change settings" src="../../../../../wheel.svg"></a></div></form></nav><section id="main-content" class="content"><div class="example-wrap"><pre class="src-line-numbers"><span id="1">1</span>
<span id="2">2</span>
<span id="3">3</span>
<span id="4">4</span>
<span id="5">5</span>
<span id="6">6</span>
<span id="7">7</span>
<span id="8">8</span>
<span id="9">9</span>
<span id="10">10</span>
<span id="11">11</span>
<span id="12">12</span>
<span id="13">13</span>
<span id="14">14</span>
<span id="15">15</span>
<span id="16">16</span>
<span id="17">17</span>
<span id="18">18</span>
<span id="19">19</span>
<span id="20">20</span>
<span id="21">21</span>
<span id="22">22</span>
<span id="23">23</span>
<span id="24">24</span>
<span id="25">25</span>
<span id="26">26</span>
<span id="27">27</span>
<span id="28">28</span>
<span id="29">29</span>
<span id="30">30</span>
<span id="31">31</span>
<span id="32">32</span>
<span id="33">33</span>
<span id="34">34</span>
<span id="35">35</span>
<span id="36">36</span>
<span id="37">37</span>
<span id="38">38</span>
<span id="39">39</span>
<span id="40">40</span>
<span id="41">41</span>
<span id="42">42</span>
<span id="43">43</span>
<span id="44">44</span>
<span id="45">45</span>
<span id="46">46</span>
<span id="47">47</span>
<span id="48">48</span>
<span id="49">49</span>
<span id="50">50</span>
<span id="51">51</span>
<span id="52">52</span>
<span id="53">53</span>
<span id="54">54</span>
<span id="55">55</span>
<span id="56">56</span>
<span id="57">57</span>
<span id="58">58</span>
<span id="59">59</span>
<span id="60">60</span>
<span id="61">61</span>
<span id="62">62</span>
<span id="63">63</span>
<span id="64">64</span>
<span id="65">65</span>
<span id="66">66</span>
<span id="67">67</span>
<span id="68">68</span>
<span id="69">69</span>
<span id="70">70</span>
<span id="71">71</span>
<span id="72">72</span>
<span id="73">73</span>
<span id="74">74</span>
<span id="75">75</span>
<span id="76">76</span>
<span id="77">77</span>
<span id="78">78</span>
<span id="79">79</span>
<span id="80">80</span>
<span id="81">81</span>
<span id="82">82</span>
<span id="83">83</span>
<span id="84">84</span>
<span id="85">85</span>
<span id="86">86</span>
<span id="87">87</span>
<span id="88">88</span>
<span id="89">89</span>
<span id="90">90</span>
<span id="91">91</span>
<span id="92">92</span>
<span id="93">93</span>
<span id="94">94</span>
<span id="95">95</span>
<span id="96">96</span>
<span id="97">97</span>
<span id="98">98</span>
<span id="99">99</span>
<span id="100">100</span>
<span id="101">101</span>
<span id="102">102</span>
<span id="103">103</span>
<span id="104">104</span>
<span id="105">105</span>
<span id="106">106</span>
<span id="107">107</span>
<span id="108">108</span>
<span id="109">109</span>
<span id="110">110</span>
<span id="111">111</span>
<span id="112">112</span>
<span id="113">113</span>
<span id="114">114</span>
<span id="115">115</span>
<span id="116">116</span>
<span id="117">117</span>
<span id="118">118</span>
<span id="119">119</span>
<span id="120">120</span>
<span id="121">121</span>
<span id="122">122</span>
<span id="123">123</span>
<span id="124">124</span>
<span id="125">125</span>
<span id="126">126</span>
<span id="127">127</span>
<span id="128">128</span>
<span id="129">129</span>
<span id="130">130</span>
<span id="131">131</span>
<span id="132">132</span>
<span id="133">133</span>
<span id="134">134</span>
<span id="135">135</span>
<span id="136">136</span>
<span id="137">137</span>
<span id="138">138</span>
<span id="139">139</span>
<span id="140">140</span>
<span id="141">141</span>
<span id="142">142</span>
<span id="143">143</span>
<span id="144">144</span>
<span id="145">145</span>
<span id="146">146</span>
<span id="147">147</span>
<span id="148">148</span>
<span id="149">149</span>
<span id="150">150</span>
<span id="151">151</span>
<span id="152">152</span>
<span id="153">153</span>
<span id="154">154</span>
<span id="155">155</span>
<span id="156">156</span>
<span id="157">157</span>
<span id="158">158</span>
<span id="159">159</span>
<span id="160">160</span>
<span id="161">161</span>
<span id="162">162</span>
<span id="163">163</span>
<span id="164">164</span>
<span id="165">165</span>
<span id="166">166</span>
<span id="167">167</span>
<span id="168">168</span>
<span id="169">169</span>
<span id="170">170</span>
<span id="171">171</span>
<span id="172">172</span>
<span id="173">173</span>
<span id="174">174</span>
<span id="175">175</span>
<span id="176">176</span>
<span id="177">177</span>
<span id="178">178</span>
<span id="179">179</span>
<span id="180">180</span>
<span id="181">181</span>
<span id="182">182</span>
<span id="183">183</span>
<span id="184">184</span>
<span id="185">185</span>
<span id="186">186</span>
<span id="187">187</span>
<span id="188">188</span>
<span id="189">189</span>
<span id="190">190</span>
<span id="191">191</span>
<span id="192">192</span>
<span id="193">193</span>
<span id="194">194</span>
<span id="195">195</span>
<span id="196">196</span>
<span id="197">197</span>
<span id="198">198</span>
<span id="199">199</span>
<span id="200">200</span>
<span id="201">201</span>
<span id="202">202</span>
<span id="203">203</span>
<span id="204">204</span>
<span id="205">205</span>
<span id="206">206</span>
<span id="207">207</span>
<span id="208">208</span>
<span id="209">209</span>
<span id="210">210</span>
<span id="211">211</span>
<span id="212">212</span>
<span id="213">213</span>
<span id="214">214</span>
<span id="215">215</span>
<span id="216">216</span>
<span id="217">217</span>
<span id="218">218</span>
<span id="219">219</span>
<span id="220">220</span>
<span id="221">221</span>
<span id="222">222</span>
<span id="223">223</span>
<span id="224">224</span>
<span id="225">225</span>
<span id="226">226</span>
<span id="227">227</span>
<span id="228">228</span>
<span id="229">229</span>
<span id="230">230</span>
<span id="231">231</span>
<span id="232">232</span>
<span id="233">233</span>
<span id="234">234</span>
<span id="235">235</span>
<span id="236">236</span>
<span id="237">237</span>
<span id="238">238</span>
<span id="239">239</span>
<span id="240">240</span>
<span id="241">241</span>
<span id="242">242</span>
<span id="243">243</span>
<span id="244">244</span>
<span id="245">245</span>
<span id="246">246</span>
<span id="247">247</span>
<span id="248">248</span>
<span id="249">249</span>
<span id="250">250</span>
<span id="251">251</span>
<span id="252">252</span>
<span id="253">253</span>
<span id="254">254</span>
<span id="255">255</span>
<span id="256">256</span>
<span id="257">257</span>
<span id="258">258</span>
<span id="259">259</span>
<span id="260">260</span>
<span id="261">261</span>
<span id="262">262</span>
<span id="263">263</span>
<span id="264">264</span>
<span id="265">265</span>
<span id="266">266</span>
<span id="267">267</span>
<span id="268">268</span>
<span id="269">269</span>
<span id="270">270</span>
<span id="271">271</span>
<span id="272">272</span>
<span id="273">273</span>
<span id="274">274</span>
<span id="275">275</span>
<span id="276">276</span>
<span id="277">277</span>
<span id="278">278</span>
<span id="279">279</span>
<span id="280">280</span>
<span id="281">281</span>
<span id="282">282</span>
<span id="283">283</span>
<span id="284">284</span>
<span id="285">285</span>
<span id="286">286</span>
<span id="287">287</span>
<span id="288">288</span>
<span id="289">289</span>
<span id="290">290</span>
<span id="291">291</span>
<span id="292">292</span>
<span id="293">293</span>
<span id="294">294</span>
<span id="295">295</span>
<span id="296">296</span>
<span id="297">297</span>
<span id="298">298</span>
<span id="299">299</span>
<span id="300">300</span>
<span id="301">301</span>
<span id="302">302</span>
<span id="303">303</span>
<span id="304">304</span>
<span id="305">305</span>
<span id="306">306</span>
<span id="307">307</span>
<span id="308">308</span>
<span id="309">309</span>
<span id="310">310</span>
<span id="311">311</span>
<span id="312">312</span>
<span id="313">313</span>
<span id="314">314</span>
<span id="315">315</span>
<span id="316">316</span>
<span id="317">317</span>
<span id="318">318</span>
<span id="319">319</span>
<span id="320">320</span>
<span id="321">321</span>
<span id="322">322</span>
<span id="323">323</span>
<span id="324">324</span>
<span id="325">325</span>
<span id="326">326</span>
<span id="327">327</span>
<span id="328">328</span>
<span id="329">329</span>
<span id="330">330</span>
<span id="331">331</span>
<span id="332">332</span>
<span id="333">333</span>
<span id="334">334</span>
<span id="335">335</span>
<span id="336">336</span>
<span id="337">337</span>
<span id="338">338</span>
<span id="339">339</span>
<span id="340">340</span>
<span id="341">341</span>
<span id="342">342</span>
<span id="343">343</span>
<span id="344">344</span>
<span id="345">345</span>
<span id="346">346</span>
<span id="347">347</span>
<span id="348">348</span>
<span id="349">349</span>
<span id="350">350</span>
<span id="351">351</span>
<span id="352">352</span>
<span id="353">353</span>
<span id="354">354</span>
<span id="355">355</span>
<span id="356">356</span>
<span id="357">357</span>
<span id="358">358</span>
<span id="359">359</span>
<span id="360">360</span>
<span id="361">361</span>
<span id="362">362</span>
<span id="363">363</span>
<span id="364">364</span>
<span id="365">365</span>
<span id="366">366</span>
<span id="367">367</span>
<span id="368">368</span>
<span id="369">369</span>
<span id="370">370</span>
<span id="371">371</span>
<span id="372">372</span>
<span id="373">373</span>
<span id="374">374</span>
<span id="375">375</span>
<span id="376">376</span>
<span id="377">377</span>
<span id="378">378</span>
<span id="379">379</span>
<span id="380">380</span>
<span id="381">381</span>
<span id="382">382</span>
<span id="383">383</span>
<span id="384">384</span>
<span id="385">385</span>
<span id="386">386</span>
<span id="387">387</span>
<span id="388">388</span>
<span id="389">389</span>
<span id="390">390</span>
<span id="391">391</span>
<span id="392">392</span>
<span id="393">393</span>
<span id="394">394</span>
<span id="395">395</span>
<span id="396">396</span>
<span id="397">397</span>
<span id="398">398</span>
<span id="399">399</span>
<span id="400">400</span>
<span id="401">401</span>
<span id="402">402</span>
<span id="403">403</span>
<span id="404">404</span>
<span id="405">405</span>
<span id="406">406</span>
<span id="407">407</span>
<span id="408">408</span>
<span id="409">409</span>
<span id="410">410</span>
<span id="411">411</span>
<span id="412">412</span>
<span id="413">413</span>
<span id="414">414</span>
<span id="415">415</span>
<span id="416">416</span>
<span id="417">417</span>
<span id="418">418</span>
<span id="419">419</span>
<span id="420">420</span>
<span id="421">421</span>
<span id="422">422</span>
<span id="423">423</span>
<span id="424">424</span>
<span id="425">425</span>
<span id="426">426</span>
<span id="427">427</span>
<span id="428">428</span>
<span id="429">429</span>
<span id="430">430</span>
<span id="431">431</span>
<span id="432">432</span>
<span id="433">433</span>
<span id="434">434</span>
<span id="435">435</span>
<span id="436">436</span>
<span id="437">437</span>
<span id="438">438</span>
<span id="439">439</span>
<span id="440">440</span>
<span id="441">441</span>
<span id="442">442</span>
<span id="443">443</span>
<span id="444">444</span>
<span id="445">445</span>
<span id="446">446</span>
<span id="447">447</span>
<span id="448">448</span>
<span id="449">449</span>
<span id="450">450</span>
<span id="451">451</span>
<span id="452">452</span>
<span id="453">453</span>
<span id="454">454</span>
<span id="455">455</span>
<span id="456">456</span>
<span id="457">457</span>
<span id="458">458</span>
<span id="459">459</span>
<span id="460">460</span>
<span id="461">461</span>
<span id="462">462</span>
<span id="463">463</span>
<span id="464">464</span>
<span id="465">465</span>
<span id="466">466</span>
<span id="467">467</span>
<span id="468">468</span>
<span id="469">469</span>
<span id="470">470</span>
<span id="471">471</span>
<span id="472">472</span>
<span id="473">473</span>
<span id="474">474</span>
<span id="475">475</span>
<span id="476">476</span>
<span id="477">477</span>
<span id="478">478</span>
<span id="479">479</span>
<span id="480">480</span>
<span id="481">481</span>
<span id="482">482</span>
<span id="483">483</span>
<span id="484">484</span>
<span id="485">485</span>
<span id="486">486</span>
<span id="487">487</span>
<span id="488">488</span>
<span id="489">489</span>
<span id="490">490</span>
<span id="491">491</span>
<span id="492">492</span>
<span id="493">493</span>
<span id="494">494</span>
<span id="495">495</span>
<span id="496">496</span>
<span id="497">497</span>
<span id="498">498</span>
<span id="499">499</span>
<span id="500">500</span>
<span id="501">501</span>
<span id="502">502</span>
<span id="503">503</span>
<span id="504">504</span>
<span id="505">505</span>
<span id="506">506</span>
<span id="507">507</span>
<span id="508">508</span>
<span id="509">509</span>
<span id="510">510</span>
<span id="511">511</span>
<span id="512">512</span>
<span id="513">513</span>
<span id="514">514</span>
<span id="515">515</span>
<span id="516">516</span>
<span id="517">517</span>
<span id="518">518</span>
<span id="519">519</span>
<span id="520">520</span>
<span id="521">521</span>
<span id="522">522</span>
<span id="523">523</span>
<span id="524">524</span>
<span id="525">525</span>
<span id="526">526</span>
<span id="527">527</span>
<span id="528">528</span>
<span id="529">529</span>
<span id="530">530</span>
<span id="531">531</span>
<span id="532">532</span>
<span id="533">533</span>
<span id="534">534</span>
<span id="535">535</span>
<span id="536">536</span>
<span id="537">537</span>
<span id="538">538</span>
<span id="539">539</span>
<span id="540">540</span>
<span id="541">541</span>
<span id="542">542</span>
<span id="543">543</span>
<span id="544">544</span>
<span id="545">545</span>
<span id="546">546</span>
<span id="547">547</span>
<span id="548">548</span>
<span id="549">549</span>
<span id="550">550</span>
<span id="551">551</span>
<span id="552">552</span>
<span id="553">553</span>
<span id="554">554</span>
<span id="555">555</span>
<span id="556">556</span>
<span id="557">557</span>
<span id="558">558</span>
<span id="559">559</span>
<span id="560">560</span>
<span id="561">561</span>
<span id="562">562</span>
<span id="563">563</span>
<span id="564">564</span>
<span id="565">565</span>
<span id="566">566</span>
<span id="567">567</span>
<span id="568">568</span>
<span id="569">569</span>
<span id="570">570</span>
<span id="571">571</span>
<span id="572">572</span>
<span id="573">573</span>
<span id="574">574</span>
<span id="575">575</span>
<span id="576">576</span>
<span id="577">577</span>
<span id="578">578</span>
<span id="579">579</span>
<span id="580">580</span>
<span id="581">581</span>
<span id="582">582</span>
<span id="583">583</span>
<span id="584">584</span>
<span id="585">585</span>
<span id="586">586</span>
<span id="587">587</span>
<span id="588">588</span>
<span id="589">589</span>
<span id="590">590</span>
<span id="591">591</span>
<span id="592">592</span>
<span id="593">593</span>
<span id="594">594</span>
<span id="595">595</span>
<span id="596">596</span>
<span id="597">597</span>
<span id="598">598</span>
<span id="599">599</span>
<span id="600">600</span>
<span id="601">601</span>
<span id="602">602</span>
<span id="603">603</span>
<span id="604">604</span>
<span id="605">605</span>
<span id="606">606</span>
<span id="607">607</span>
<span id="608">608</span>
<span id="609">609</span>
<span id="610">610</span>
<span id="611">611</span>
<span id="612">612</span>
<span id="613">613</span>
<span id="614">614</span>
<span id="615">615</span>
<span id="616">616</span>
<span id="617">617</span>
<span id="618">618</span>
<span id="619">619</span>
<span id="620">620</span>
<span id="621">621</span>
<span id="622">622</span>
<span id="623">623</span>
<span id="624">624</span>
<span id="625">625</span>
<span id="626">626</span>
<span id="627">627</span>
<span id="628">628</span>
<span id="629">629</span>
<span id="630">630</span>
<span id="631">631</span>
<span id="632">632</span>
<span id="633">633</span>
<span id="634">634</span>
<span id="635">635</span>
<span id="636">636</span>
<span id="637">637</span>
<span id="638">638</span>
<span id="639">639</span>
<span id="640">640</span>
<span id="641">641</span>
<span id="642">642</span>
<span id="643">643</span>
<span id="644">644</span>
<span id="645">645</span>
<span id="646">646</span>
<span id="647">647</span>
<span id="648">648</span>
<span id="649">649</span>
<span id="650">650</span>
<span id="651">651</span>
<span id="652">652</span>
<span id="653">653</span>
<span id="654">654</span>
<span id="655">655</span>
<span id="656">656</span>
<span id="657">657</span>
<span id="658">658</span>
<span id="659">659</span>
<span id="660">660</span>
<span id="661">661</span>
<span id="662">662</span>
<span id="663">663</span>
<span id="664">664</span>
<span id="665">665</span>
<span id="666">666</span>
<span id="667">667</span>
<span id="668">668</span>
<span id="669">669</span>
<span id="670">670</span>
<span id="671">671</span>
<span id="672">672</span>
<span id="673">673</span>
<span id="674">674</span>
<span id="675">675</span>
<span id="676">676</span>
<span id="677">677</span>
<span id="678">678</span>
<span id="679">679</span>
<span id="680">680</span>
<span id="681">681</span>
<span id="682">682</span>
<span id="683">683</span>
<span id="684">684</span>
<span id="685">685</span>
<span id="686">686</span>
<span id="687">687</span>
<span id="688">688</span>
<span id="689">689</span>
<span id="690">690</span>
<span id="691">691</span>
<span id="692">692</span>
<span id="693">693</span>
<span id="694">694</span>
<span id="695">695</span>
<span id="696">696</span>
<span id="697">697</span>
<span id="698">698</span>
<span id="699">699</span>
<span id="700">700</span>
<span id="701">701</span>
<span id="702">702</span>
<span id="703">703</span>
<span id="704">704</span>
<span id="705">705</span>
<span id="706">706</span>
<span id="707">707</span>
<span id="708">708</span>
<span id="709">709</span>
<span id="710">710</span>
<span id="711">711</span>
<span id="712">712</span>
<span id="713">713</span>
<span id="714">714</span>
<span id="715">715</span>
<span id="716">716</span>
<span id="717">717</span>
<span id="718">718</span>
<span id="719">719</span>
<span id="720">720</span>
<span id="721">721</span>
<span id="722">722</span>
<span id="723">723</span>
<span id="724">724</span>
<span id="725">725</span>
<span id="726">726</span>
<span id="727">727</span>
<span id="728">728</span>
<span id="729">729</span>
<span id="730">730</span>
<span id="731">731</span>
<span id="732">732</span>
<span id="733">733</span>
<span id="734">734</span>
<span id="735">735</span>
<span id="736">736</span>
<span id="737">737</span>
<span id="738">738</span>
<span id="739">739</span>
<span id="740">740</span>
<span id="741">741</span>
<span id="742">742</span>
<span id="743">743</span>
<span id="744">744</span>
<span id="745">745</span>
<span id="746">746</span>
<span id="747">747</span>
<span id="748">748</span>
<span id="749">749</span>
<span id="750">750</span>
<span id="751">751</span>
<span id="752">752</span>
<span id="753">753</span>
<span id="754">754</span>
<span id="755">755</span>
<span id="756">756</span>
<span id="757">757</span>
<span id="758">758</span>
<span id="759">759</span>
<span id="760">760</span>
<span id="761">761</span>
<span id="762">762</span>
<span id="763">763</span>
<span id="764">764</span>
<span id="765">765</span>
<span id="766">766</span>
<span id="767">767</span>
<span id="768">768</span>
<span id="769">769</span>
<span id="770">770</span>
<span id="771">771</span>
<span id="772">772</span>
<span id="773">773</span>
<span id="774">774</span>
<span id="775">775</span>
<span id="776">776</span>
<span id="777">777</span>
<span id="778">778</span>
<span id="779">779</span>
<span id="780">780</span>
<span id="781">781</span>
<span id="782">782</span>
<span id="783">783</span>
<span id="784">784</span>
<span id="785">785</span>
<span id="786">786</span>
<span id="787">787</span>
<span id="788">788</span>
<span id="789">789</span>
<span id="790">790</span>
<span id="791">791</span>
<span id="792">792</span>
<span id="793">793</span>
<span id="794">794</span>
<span id="795">795</span>
<span id="796">796</span>
<span id="797">797</span>
<span id="798">798</span>
<span id="799">799</span>
<span id="800">800</span>
<span id="801">801</span>
<span id="802">802</span>
<span id="803">803</span>
<span id="804">804</span>
<span id="805">805</span>
<span id="806">806</span>
<span id="807">807</span>
<span id="808">808</span>
<span id="809">809</span>
<span id="810">810</span>
<span id="811">811</span>
<span id="812">812</span>
<span id="813">813</span>
<span id="814">814</span>
<span id="815">815</span>
</pre><pre class="rust"><code><span class="kw">use </span><span class="kw">super</span>::metadata::<span class="kw-2">*</span>;
<span class="kw">use </span><span class="kw">crate</span>::plan::ObjectQueue;
<span class="kw">use </span><span class="kw">crate</span>::plan::VectorObjectQueue;
<span class="kw">use </span><span class="kw">crate</span>::policy::sft::GCWorkerMutRef;
<span class="kw">use </span><span class="kw">crate</span>::policy::sft::SFT;
<span class="kw">use </span><span class="kw">crate</span>::policy::space::CommonSpace;
<span class="kw">use </span><span class="kw">crate</span>::scheduler::GCWorkScheduler;
<span class="kw">use </span><span class="kw">crate</span>::util::heap::gc_trigger::GCTrigger;
<span class="kw">use </span><span class="kw">crate</span>::util::heap::PageResource;
<span class="kw">use </span><span class="kw">crate</span>::util::malloc::library::{BYTES_IN_MALLOC_PAGE, LOG_BYTES_IN_MALLOC_PAGE};
<span class="kw">use </span><span class="kw">crate</span>::util::malloc::malloc_ms_util::<span class="kw-2">*</span>;
<span class="kw">use </span><span class="kw">crate</span>::util::metadata::side_metadata::{
    SideMetadataContext, SideMetadataSanity, SideMetadataSpec,
};
<span class="kw">use </span><span class="kw">crate</span>::util::metadata::MetadataSpec;
<span class="kw">use </span><span class="kw">crate</span>::util::opaque_pointer::<span class="kw-2">*</span>;
<span class="kw">use </span><span class="kw">crate</span>::util::Address;
<span class="kw">use </span><span class="kw">crate</span>::util::ObjectReference;
<span class="kw">use </span><span class="kw">crate</span>::util::{conversions, metadata};
<span class="kw">use </span><span class="kw">crate</span>::vm::VMBinding;
<span class="kw">use </span><span class="kw">crate</span>::vm::{ActivePlan, Collection, ObjectModel};
<span class="kw">use crate</span>::{policy::space::Space, util::heap::layout::vm_layout_constants::BYTES_IN_CHUNK};
<span class="attribute">#[cfg(debug_assertions)]
</span><span class="kw">use </span>std::collections::HashMap;
<span class="kw">use </span>std::marker::PhantomData;
<span class="attribute">#[cfg(debug_assertions)]
</span><span class="kw">use </span>std::sync::atomic::AtomicU32;
<span class="kw">use </span>std::sync::atomic::{AtomicUsize, Ordering};
<span class="kw">use </span>std::sync::Arc;
<span class="attribute">#[cfg(debug_assertions)]
</span><span class="kw">use </span>std::sync::Mutex;
<span class="comment">// If true, we will use a hashmap to store all the allocated memory from malloc, and use it
// to make sure our allocation is correct.
</span><span class="attribute">#[cfg(debug_assertions)]
</span><span class="kw">const </span>ASSERT_ALLOCATION: bool = <span class="bool-val">false</span>;

<span class="doccomment">/// This space uses malloc to get new memory, and performs mark-sweep for the memory.
</span><span class="kw">pub struct </span>MallocSpace&lt;VM: VMBinding&gt; {
    phantom: PhantomData&lt;VM&gt;,
    active_bytes: AtomicUsize,
    active_pages: AtomicUsize,
    <span class="kw">pub </span>chunk_addr_min: AtomicUsize, <span class="comment">// XXX: have to use AtomicUsize to represent an Address
    </span><span class="kw">pub </span>chunk_addr_max: AtomicUsize,
    metadata: SideMetadataContext,
    <span class="doccomment">/// Work packet scheduler
    </span>scheduler: Arc&lt;GCWorkScheduler&lt;VM&gt;&gt;,
    gc_trigger: Arc&lt;GCTrigger&lt;VM&gt;&gt;,
    <span class="comment">// Mapping between allocated address and its size - this is used to check correctness.
    // Size will be set to zero when the memory is freed.
    </span><span class="attribute">#[cfg(debug_assertions)]
    </span>active_mem: Mutex&lt;HashMap&lt;Address, usize&gt;&gt;,
    <span class="comment">// The following fields are used for checking correctness of the parallel sweep implementation
    // as we need to check how many live bytes exist against `active_bytes` when the last sweep
    // work packet is executed
    </span><span class="attribute">#[cfg(debug_assertions)]
    </span><span class="kw">pub </span>total_work_packets: AtomicU32,
    <span class="attribute">#[cfg(debug_assertions)]
    </span><span class="kw">pub </span>completed_work_packets: AtomicU32,
    <span class="attribute">#[cfg(debug_assertions)]
    </span><span class="kw">pub </span>work_live_bytes: AtomicUsize,
}

<span class="kw">impl</span>&lt;VM: VMBinding&gt; SFT <span class="kw">for </span>MallocSpace&lt;VM&gt; {
    <span class="kw">fn </span>name(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span>str {
        <span class="self">self</span>.get_name()
    }

    <span class="kw">fn </span>is_live(<span class="kw-2">&amp;</span><span class="self">self</span>, object: ObjectReference) -&gt; bool {
        is_marked::&lt;VM&gt;(object, Ordering::SeqCst)
    }

    <span class="attribute">#[cfg(feature = <span class="string">&quot;object_pinning&quot;</span>)]
    </span><span class="kw">fn </span>pin_object(<span class="kw-2">&amp;</span><span class="self">self</span>, _object: ObjectReference) -&gt; bool {
        <span class="bool-val">false
    </span>}

    <span class="attribute">#[cfg(feature = <span class="string">&quot;object_pinning&quot;</span>)]
    </span><span class="kw">fn </span>unpin_object(<span class="kw-2">&amp;</span><span class="self">self</span>, _object: ObjectReference) -&gt; bool {
        <span class="bool-val">false
    </span>}

    <span class="attribute">#[cfg(feature = <span class="string">&quot;object_pinning&quot;</span>)]
    </span><span class="kw">fn </span>is_object_pinned(<span class="kw-2">&amp;</span><span class="self">self</span>, _object: ObjectReference) -&gt; bool {
        <span class="bool-val">false
    </span>}

    <span class="kw">fn </span>is_movable(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; bool {
        <span class="bool-val">false
    </span>}

    <span class="attribute">#[cfg(feature = <span class="string">&quot;sanity&quot;</span>)]
    </span><span class="kw">fn </span>is_sane(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; bool {
        <span class="bool-val">true
    </span>}

    <span class="comment">// For malloc space, we need to further check the alloc bit.
    </span><span class="kw">fn </span>is_in_space(<span class="kw-2">&amp;</span><span class="self">self</span>, object: ObjectReference) -&gt; bool {
        is_alloced_by_malloc::&lt;VM&gt;(object)
    }

    <span class="doccomment">/// For malloc space, we just use the side metadata.
    </span><span class="attribute">#[cfg(feature = <span class="string">&quot;is_mmtk_object&quot;</span>)]
    </span><span class="kw">fn </span>is_mmtk_object(<span class="kw-2">&amp;</span><span class="self">self</span>, addr: Address) -&gt; bool {
        <span class="macro">debug_assert!</span>(!addr.is_zero());
        <span class="comment">// `addr` cannot be mapped by us. It should be mapped by the malloc library.
        </span><span class="macro">debug_assert!</span>(!addr.is_mapped());
        has_object_alloced_by_malloc::&lt;VM&gt;(addr).is_some()
    }

    <span class="kw">fn </span>initialize_object_metadata(<span class="kw-2">&amp;</span><span class="self">self</span>, object: ObjectReference, _alloc: bool) {
        <span class="macro">trace!</span>(<span class="string">&quot;initialize_object_metadata for object {}&quot;</span>, object);
        set_alloc_bit::&lt;VM&gt;(object);
    }

    <span class="kw">fn </span>sft_trace_object(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        queue: <span class="kw-2">&amp;mut </span>VectorObjectQueue,
        object: ObjectReference,
        _worker: GCWorkerMutRef,
    ) -&gt; ObjectReference {
        <span class="self">self</span>.trace_object(queue, object)
    }
}

<span class="kw">impl</span>&lt;VM: VMBinding&gt; Space&lt;VM&gt; <span class="kw">for </span>MallocSpace&lt;VM&gt; {
    <span class="kw">fn </span>as_space(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span><span class="kw">dyn </span>Space&lt;VM&gt; {
        <span class="self">self
    </span>}

    <span class="kw">fn </span>as_sft(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span>(<span class="kw">dyn </span>SFT + Sync + <span class="lifetime">&#39;static</span>) {
        <span class="self">self
    </span>}

    <span class="kw">fn </span>get_page_resource(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span><span class="kw">dyn </span>PageResource&lt;VM&gt; {
        <span class="macro">unreachable!</span>()
    }

    <span class="kw">fn </span>common(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span>CommonSpace&lt;VM&gt; {
        <span class="macro">unreachable!</span>()
    }

    <span class="kw">fn </span>get_gc_trigger(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span>GCTrigger&lt;VM&gt; {
        <span class="self">self</span>.gc_trigger.as_ref()
    }

    <span class="kw">fn </span>initialize_sft(<span class="kw-2">&amp;</span><span class="self">self</span>) {
        <span class="comment">// Do nothing - we will set sft when we get new results from malloc
    </span>}

    <span class="kw">fn </span>release_multiple_pages(<span class="kw-2">&amp;mut </span><span class="self">self</span>, _start: Address) {
        <span class="macro">unreachable!</span>()
    }

    <span class="comment">// We have assertions in a debug build. We allow this pattern for the release build.
    </span><span class="attribute">#[allow(clippy::let_and_return)]
    </span><span class="kw">fn </span>in_space(<span class="kw-2">&amp;</span><span class="self">self</span>, object: ObjectReference) -&gt; bool {
        <span class="kw">let </span>ret = is_alloced_by_malloc::&lt;VM&gt;(object);

        <span class="attribute">#[cfg(debug_assertions)]
        </span><span class="kw">if </span>ASSERT_ALLOCATION {
            <span class="kw">let </span>addr = object.to_object_start::&lt;VM&gt;();
            <span class="kw">let </span>active_mem = <span class="self">self</span>.active_mem.lock().unwrap();
            <span class="kw">if </span>ret {
                <span class="comment">// The alloc bit tells that the object is in space.
                </span><span class="macro">debug_assert!</span>(
                    <span class="kw-2">*</span>active_mem.get(<span class="kw-2">&amp;</span>addr).unwrap() != <span class="number">0</span>,
                    <span class="string">&quot;active mem check failed for {} (object {}) - was freed&quot;</span>,
                    addr,
                    object
                );
            } <span class="kw">else </span>{
                <span class="comment">// The alloc bit tells that the object is not in space. It could never be allocated, or have been freed.
                </span><span class="macro">debug_assert!</span>(
                    (!active_mem.contains_key(<span class="kw-2">&amp;</span>addr))
                        || (active_mem.contains_key(<span class="kw-2">&amp;</span>addr) &amp;&amp; <span class="kw-2">*</span>active_mem.get(<span class="kw-2">&amp;</span>addr).unwrap() == <span class="number">0</span>),
                    <span class="string">&quot;mem check failed for {} (object {}): allocated = {}, size = {:?}&quot;</span>,
                    addr,
                    object,
                    active_mem.contains_key(<span class="kw-2">&amp;</span>addr),
                    <span class="kw">if </span>active_mem.contains_key(<span class="kw-2">&amp;</span>addr) {
                        active_mem.get(<span class="kw-2">&amp;</span>addr)
                    } <span class="kw">else </span>{
                        <span class="prelude-val">None
                    </span>}
                );
            }
        }
        ret
    }

    <span class="kw">fn </span>address_in_space(<span class="kw-2">&amp;</span><span class="self">self</span>, _start: Address) -&gt; bool {
        <span class="macro">unreachable!</span>(<span class="string">&quot;We do not know if an address is in malloc space. Use in_space() to check if an object is in malloc space.&quot;</span>)
    }

    <span class="kw">fn </span>get_name(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; <span class="kw-2">&amp;</span><span class="lifetime">&#39;static </span>str {
        <span class="string">&quot;MallocSpace&quot;
    </span>}

    <span class="attribute">#[allow(clippy::assertions_on_constants)]
    </span><span class="kw">fn </span>reserved_pages(<span class="kw-2">&amp;</span><span class="self">self</span>) -&gt; usize {
        <span class="kw">use </span><span class="kw">crate</span>::util::constants::LOG_BYTES_IN_PAGE;
        <span class="comment">// Assume malloc pages are no smaller than 4K pages. Otherwise the substraction below will fail.
        </span><span class="macro">debug_assert!</span>(LOG_BYTES_IN_MALLOC_PAGE &gt;= LOG_BYTES_IN_PAGE);
        <span class="kw">let </span>data_pages = <span class="self">self</span>.active_pages.load(Ordering::SeqCst)
            &lt;&lt; (LOG_BYTES_IN_MALLOC_PAGE - LOG_BYTES_IN_PAGE);
        <span class="kw">let </span>meta_pages = <span class="self">self</span>.metadata.calculate_reserved_pages(data_pages);
        data_pages + meta_pages
    }

    <span class="kw">fn </span>verify_side_metadata_sanity(<span class="kw-2">&amp;</span><span class="self">self</span>, side_metadata_sanity_checker: <span class="kw-2">&amp;mut </span>SideMetadataSanity) {
        side_metadata_sanity_checker
            .verify_metadata_context(std::any::type_name::&lt;<span class="self">Self</span>&gt;(), <span class="kw-2">&amp;</span><span class="self">self</span>.metadata)
    }
}

<span class="kw">use </span><span class="kw">crate</span>::scheduler::GCWorker;
<span class="kw">use </span><span class="kw">crate</span>::util::copy::CopySemantics;

<span class="kw">impl</span>&lt;VM: VMBinding&gt; <span class="kw">crate</span>::policy::gc_work::PolicyTraceObject&lt;VM&gt; <span class="kw">for </span>MallocSpace&lt;VM&gt; {
    <span class="kw">fn </span>trace_object&lt;Q: ObjectQueue, <span class="kw">const </span>KIND: <span class="kw">crate</span>::policy::gc_work::TraceKind&gt;(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        queue: <span class="kw-2">&amp;mut </span>Q,
        object: ObjectReference,
        _copy: <span class="prelude-ty">Option</span>&lt;CopySemantics&gt;,
        _worker: <span class="kw-2">&amp;mut </span>GCWorker&lt;VM&gt;,
    ) -&gt; ObjectReference {
        <span class="self">self</span>.trace_object(queue, object)
    }

    <span class="kw">fn </span>may_move_objects&lt;<span class="kw">const </span>KIND: <span class="kw">crate</span>::policy::gc_work::TraceKind&gt;() -&gt; bool {
        <span class="bool-val">false
    </span>}
}

<span class="comment">// Actually no max object size.
</span><span class="attribute">#[allow(dead_code)]
</span><span class="kw">pub const </span>MAX_OBJECT_SIZE: usize = <span class="kw">crate</span>::util::constants::MAX_INT;

<span class="kw">impl</span>&lt;VM: VMBinding&gt; MallocSpace&lt;VM&gt; {
    <span class="kw">pub fn </span>extend_global_side_metadata_specs(specs: <span class="kw-2">&amp;mut </span>Vec&lt;SideMetadataSpec&gt;) {
        <span class="comment">// MallocSpace needs to use alloc bit. If the feature is turned on, the alloc bit spec is in the global specs.
        // Otherwise, we manually add it.
        </span><span class="kw">if </span>!<span class="macro">cfg!</span>(feature = <span class="string">&quot;global_alloc_bit&quot;</span>) {
            specs.push(<span class="kw">crate</span>::util::alloc_bit::ALLOC_SIDE_METADATA_SPEC);
        }
        <span class="comment">// MallocSpace also need a global chunk metadata.
        // TODO: I don&#39;t know why this is a global spec. Can we replace it with the chunk map (and the local spec used in the chunk map)?
        // One reason could be that the address range in this space is not in our control, and it could be anywhere in the heap, thus we have
        // to make it a global spec. I am not too sure about this.
        </span>specs.push(ACTIVE_CHUNK_METADATA_SPEC);
    }

    <span class="kw">pub fn </span>new(args: <span class="kw">crate</span>::policy::space::PlanCreateSpaceArgs&lt;VM&gt;) -&gt; <span class="self">Self </span>{
        MallocSpace {
            phantom: PhantomData,
            active_bytes: AtomicUsize::new(<span class="number">0</span>),
            active_pages: AtomicUsize::new(<span class="number">0</span>),
            chunk_addr_min: AtomicUsize::new(usize::max_value()), <span class="comment">// XXX: have to use AtomicUsize to represent an Address
            </span>chunk_addr_max: AtomicUsize::new(<span class="number">0</span>),
            metadata: SideMetadataContext {
                global: args.global_side_metadata_specs.clone(),
                local: metadata::extract_side_metadata(<span class="kw-2">&amp;</span>[
                    MetadataSpec::OnSide(ACTIVE_PAGE_METADATA_SPEC),
                    MetadataSpec::OnSide(OFFSET_MALLOC_METADATA_SPEC),
                    <span class="kw-2">*</span>VM::VMObjectModel::LOCAL_MARK_BIT_SPEC,
                ]),
            },
            scheduler: args.scheduler.clone(),
            gc_trigger: args.gc_trigger,
            <span class="attribute">#[cfg(debug_assertions)]
            </span>active_mem: Mutex::new(HashMap::new()),
            <span class="attribute">#[cfg(debug_assertions)]
            </span>total_work_packets: AtomicU32::new(<span class="number">0</span>),
            <span class="attribute">#[cfg(debug_assertions)]
            </span>completed_work_packets: AtomicU32::new(<span class="number">0</span>),
            <span class="attribute">#[cfg(debug_assertions)]
            </span>work_live_bytes: AtomicUsize::new(<span class="number">0</span>),
        }
    }

    <span class="doccomment">/// Set multiple pages, starting from the given address, for the given size, and increase the active page count if we set any page mark in the region.
    /// This is a thread-safe method, and can be used during mutator phase when mutators may access the same page.
    /// Performance-wise, this method may impose overhead, as we are doing a compare-exchange for every page in the range.
    </span><span class="kw">fn </span>set_page_mark(<span class="kw-2">&amp;</span><span class="self">self</span>, start: Address, size: usize) {
        <span class="comment">// Set first page
        </span><span class="kw">let </span><span class="kw-2">mut </span>page = start.align_down(BYTES_IN_MALLOC_PAGE);
        <span class="kw">let </span><span class="kw-2">mut </span>used_pages = <span class="number">0</span>;

        <span class="comment">// It is important to go to the end of the object, which may span a page boundary
        </span><span class="kw">while </span>page &lt; start + size {
            <span class="kw">if </span>compare_exchange_set_page_mark(page) {
                used_pages += <span class="number">1</span>;
            }

            page += BYTES_IN_MALLOC_PAGE;
        }

        <span class="kw">if </span>used_pages != <span class="number">0 </span>{
            <span class="self">self</span>.active_pages.fetch_add(used_pages, Ordering::SeqCst);
        }
    }

    <span class="doccomment">/// Unset multiple pages, starting from the given address, for the given size, and decrease the active page count if we unset any page mark in the region
    ///
    /// # Safety
    /// We need to ensure that only one GC thread is accessing the range.
    </span><span class="kw">unsafe fn </span>unset_page_mark(<span class="kw-2">&amp;</span><span class="self">self</span>, start: Address, size: usize) {
        <span class="macro">debug_assert!</span>(start.is_aligned_to(BYTES_IN_MALLOC_PAGE));
        <span class="macro">debug_assert!</span>(<span class="kw">crate</span>::util::conversions::raw_is_aligned(
            size,
            BYTES_IN_MALLOC_PAGE
        ));
        <span class="kw">let </span><span class="kw-2">mut </span>page = start;
        <span class="kw">let </span><span class="kw-2">mut </span>cleared_pages = <span class="number">0</span>;
        <span class="kw">while </span>page &lt; start + size {
            <span class="kw">if </span>is_page_marked_unsafe(page) {
                cleared_pages += <span class="number">1</span>;
                unset_page_mark_unsafe(page);
            }
            page += BYTES_IN_MALLOC_PAGE;
        }

        <span class="kw">if </span>cleared_pages != <span class="number">0 </span>{
            <span class="self">self</span>.active_pages.fetch_sub(cleared_pages, Ordering::SeqCst);
        }
    }

    <span class="kw">pub fn </span>alloc(<span class="kw-2">&amp;</span><span class="self">self</span>, tls: VMThread, size: usize, align: usize, offset: isize) -&gt; Address {
        <span class="comment">// TODO: Should refactor this and Space.acquire()
        </span><span class="kw">if </span><span class="self">self</span>.get_gc_trigger().poll(<span class="bool-val">false</span>, <span class="prelude-val">Some</span>(<span class="self">self</span>)) {
            <span class="macro">assert!</span>(VM::VMActivePlan::is_mutator(tls), <span class="string">&quot;Polling in GC worker&quot;</span>);
            VM::VMCollection::block_for_gc(VMMutatorThread(tls));
            <span class="kw">return unsafe </span>{ Address::zero() };
        }

        <span class="kw">let </span>(address, is_offset_malloc) = alloc::&lt;VM&gt;(size, align, offset);
        <span class="kw">if </span>!address.is_zero() {
            <span class="kw">let </span>actual_size = get_malloc_usable_size(address, is_offset_malloc);

            <span class="comment">// If the side metadata for the address has not yet been mapped, we will map all the side metadata for the range [address, address + actual_size).
            </span><span class="kw">if </span>!is_meta_space_mapped(address, actual_size) {
                <span class="comment">// Map the metadata space for the associated chunk
                </span><span class="self">self</span>.map_metadata_and_update_bound(address, actual_size);
                <span class="comment">// Update SFT
                </span><span class="macro">assert!</span>(<span class="kw">crate</span>::mmtk::SFT_MAP.has_sft_entry(address)); <span class="comment">// make sure the address is okay with our SFT map
                </span><span class="kw">unsafe </span>{ <span class="kw">crate</span>::mmtk::SFT_MAP.update(<span class="self">self</span>, address, actual_size) };
            }

            <span class="comment">// Set page marks for current object
            </span><span class="self">self</span>.set_page_mark(address, actual_size);
            <span class="self">self</span>.active_bytes.fetch_add(actual_size, Ordering::SeqCst);

            <span class="kw">if </span>is_offset_malloc {
                set_offset_malloc_bit(address);
            }

            <span class="attribute">#[cfg(debug_assertions)]
            </span><span class="kw">if </span>ASSERT_ALLOCATION {
                <span class="macro">debug_assert!</span>(actual_size != <span class="number">0</span>);
                <span class="self">self</span>.active_mem.lock().unwrap().insert(address, actual_size);
            }
        }

        address
    }

    <span class="kw">pub fn </span>free(<span class="kw-2">&amp;</span><span class="self">self</span>, addr: Address) {
        <span class="kw">let </span>offset_malloc_bit = is_offset_malloc(addr);
        <span class="kw">let </span>bytes = get_malloc_usable_size(addr, offset_malloc_bit);
        <span class="self">self</span>.free_internal(addr, bytes, offset_malloc_bit);
    }

    <span class="comment">// XXX optimize: We pass the bytes in to free as otherwise there were multiple
    // indirect call instructions in the generated assembly
    </span><span class="kw">fn </span>free_internal(<span class="kw-2">&amp;</span><span class="self">self</span>, addr: Address, bytes: usize, offset_malloc_bit: bool) {
        <span class="kw">if </span>offset_malloc_bit {
            <span class="macro">trace!</span>(<span class="string">&quot;Free memory {:x}&quot;</span>, addr);
            offset_free(addr);
            <span class="kw">unsafe </span>{ unset_offset_malloc_bit_unsafe(addr) };
        } <span class="kw">else </span>{
            <span class="kw">let </span>ptr = addr.to_mut_ptr();
            <span class="macro">trace!</span>(<span class="string">&quot;Free memory {:?}&quot;</span>, ptr);
            <span class="kw">unsafe </span>{
                free(ptr);
            }
        }

        <span class="self">self</span>.active_bytes.fetch_sub(bytes, Ordering::SeqCst);

        <span class="attribute">#[cfg(debug_assertions)]
        </span><span class="kw">if </span>ASSERT_ALLOCATION {
            <span class="self">self</span>.active_mem.lock().unwrap().insert(addr, <span class="number">0</span>).unwrap();
        }
    }

    <span class="kw">pub fn </span>trace_object&lt;Q: ObjectQueue&gt;(
        <span class="kw-2">&amp;</span><span class="self">self</span>,
        queue: <span class="kw-2">&amp;mut </span>Q,
        object: ObjectReference,
    ) -&gt; ObjectReference {
        <span class="kw">if </span>object.is_null() {
            <span class="kw">return </span>object;
        }

        <span class="macro">assert!</span>(
            <span class="self">self</span>.in_space(object),
            <span class="string">&quot;Cannot mark an object {} that was not alloced by malloc.&quot;</span>,
            object,
        );

        <span class="kw">if </span>!is_marked::&lt;VM&gt;(object, Ordering::Relaxed) {
            <span class="kw">let </span>chunk_start = conversions::chunk_align_down(object.to_object_start::&lt;VM&gt;());
            set_mark_bit::&lt;VM&gt;(object, Ordering::SeqCst);
            set_chunk_mark(chunk_start);
            queue.enqueue(object);
        }

        object
    }

    <span class="kw">fn </span>map_metadata_and_update_bound(<span class="kw-2">&amp;</span><span class="self">self</span>, addr: Address, size: usize) {
        <span class="comment">// Map the metadata space for the range [addr, addr + size)
        </span>map_meta_space(<span class="kw-2">&amp;</span><span class="self">self</span>.metadata, addr, size);

        <span class="comment">// Update the bounds of the max and min chunk addresses seen -- this is used later in the sweep
        // Lockless compare-and-swap loops perform better than a locking variant

        // Update chunk_addr_min, basing on the start of the allocation: addr.
        </span>{
            <span class="kw">let </span>min_chunk_start = conversions::chunk_align_down(addr);
            <span class="kw">let </span>min_chunk_usize = min_chunk_start.as_usize();
            <span class="kw">let </span><span class="kw-2">mut </span>min = <span class="self">self</span>.chunk_addr_min.load(Ordering::Relaxed);
            <span class="kw">while </span>min_chunk_usize &lt; min {
                <span class="kw">match </span><span class="self">self</span>.chunk_addr_min.compare_exchange_weak(
                    min,
                    min_chunk_usize,
                    Ordering::AcqRel,
                    Ordering::Relaxed,
                ) {
                    <span class="prelude-val">Ok</span>(<span class="kw">_</span>) =&gt; <span class="kw">break</span>,
                    <span class="prelude-val">Err</span>(x) =&gt; min = x,
                }
            }
        }

        <span class="comment">// Update chunk_addr_max, basing on the end of the allocation: addr + size.
        </span>{
            <span class="kw">let </span>max_chunk_start = conversions::chunk_align_down(addr + size);
            <span class="kw">let </span>max_chunk_usize = max_chunk_start.as_usize();
            <span class="kw">let </span><span class="kw-2">mut </span>max = <span class="self">self</span>.chunk_addr_max.load(Ordering::Relaxed);
            <span class="kw">while </span>max_chunk_usize &gt; max {
                <span class="kw">match </span><span class="self">self</span>.chunk_addr_max.compare_exchange_weak(
                    max,
                    max_chunk_usize,
                    Ordering::AcqRel,
                    Ordering::Relaxed,
                ) {
                    <span class="prelude-val">Ok</span>(<span class="kw">_</span>) =&gt; <span class="kw">break</span>,
                    <span class="prelude-val">Err</span>(x) =&gt; max = x,
                }
            }
        }
    }

    <span class="kw">pub fn </span>prepare(<span class="kw-2">&amp;mut </span><span class="self">self</span>) {}

    <span class="kw">pub fn </span>release(<span class="kw-2">&amp;mut </span><span class="self">self</span>) {
        <span class="kw">use </span><span class="kw">crate</span>::scheduler::WorkBucketStage;
        <span class="kw">let </span><span class="kw-2">mut </span>work_packets: Vec&lt;Box&lt;<span class="kw">dyn </span>GCWork&lt;VM&gt;&gt;&gt; = <span class="macro">vec!</span>[];
        <span class="kw">let </span><span class="kw-2">mut </span>chunk = <span class="kw">unsafe </span>{ Address::from_usize(<span class="self">self</span>.chunk_addr_min.load(Ordering::Relaxed)) }; <span class="comment">// XXX: have to use AtomicUsize to represent an Address
        </span><span class="kw">let </span>end = <span class="kw">unsafe </span>{ Address::from_usize(<span class="self">self</span>.chunk_addr_max.load(Ordering::Relaxed)) }
            + BYTES_IN_CHUNK;

        <span class="comment">// Since only a single thread generates the sweep work packets as well as it is a Stop-the-World collector,
        // we can assume that the chunk mark metadata is not being accessed by anything else and hence we use
        // non-atomic accesses
        </span><span class="kw">let </span>space = <span class="kw">unsafe </span>{ <span class="kw-2">&amp;*</span>(<span class="self">self </span><span class="kw">as </span><span class="kw-2">*const </span><span class="self">Self</span>) };
        <span class="kw">while </span>chunk &lt; end {
            <span class="kw">if </span>is_chunk_mapped(chunk) &amp;&amp; <span class="kw">unsafe </span>{ is_chunk_marked_unsafe(chunk) } {
                work_packets.push(Box::new(MSSweepChunk { ms: space, chunk }));
            }

            chunk += BYTES_IN_CHUNK;
        }

        <span class="macro">debug!</span>(<span class="string">&quot;Generated {} sweep work packets&quot;</span>, work_packets.len());
        <span class="attribute">#[cfg(debug_assertions)]
        </span>{
            <span class="self">self</span>.total_work_packets
                .store(work_packets.len() <span class="kw">as </span>u32, Ordering::SeqCst);
            <span class="self">self</span>.completed_work_packets.store(<span class="number">0</span>, Ordering::SeqCst);
            <span class="self">self</span>.work_live_bytes.store(<span class="number">0</span>, Ordering::SeqCst);
        }

        <span class="self">self</span>.scheduler.work_buckets[WorkBucketStage::Release].bulk_add(work_packets);
    }

    <span class="kw">pub fn </span>sweep_chunk(<span class="kw-2">&amp;</span><span class="self">self</span>, chunk_start: Address) {
        <span class="comment">// Call the relevant sweep function depending on the location of the mark bits
        </span><span class="kw">match </span><span class="kw-2">*</span>VM::VMObjectModel::LOCAL_MARK_BIT_SPEC {
            MetadataSpec::OnSide(local_mark_bit_side_spec) =&gt; {
                <span class="self">self</span>.sweep_chunk_mark_on_side(chunk_start, local_mark_bit_side_spec);
            }
            <span class="kw">_ </span>=&gt; {
                <span class="self">self</span>.sweep_chunk_mark_in_header(chunk_start);
            }
        }
    }

    <span class="doccomment">/// Given an object in MallocSpace, return its malloc address, whether it is an offset malloc, and malloc size
    </span><span class="kw">fn </span>get_malloc_addr_size(object: ObjectReference) -&gt; (Address, bool, usize) {
        <span class="kw">let </span>obj_start = object.to_object_start::&lt;VM&gt;();
        <span class="kw">let </span>offset_malloc_bit = is_offset_malloc(obj_start);
        <span class="kw">let </span>bytes = get_malloc_usable_size(obj_start, offset_malloc_bit);
        (obj_start, offset_malloc_bit, bytes)
    }

    <span class="doccomment">/// Clean up for an empty chunk
    </span><span class="kw">fn </span>clean_up_empty_chunk(<span class="kw-2">&amp;</span><span class="self">self</span>, chunk_start: Address) {
        <span class="comment">// Since the chunk mark metadata is a byte, we don&#39;t need synchronization
        </span><span class="kw">unsafe </span>{ unset_chunk_mark_unsafe(chunk_start) };
        <span class="comment">// Clear the SFT entry
        </span><span class="kw">unsafe </span>{ <span class="kw">crate</span>::mmtk::SFT_MAP.clear(chunk_start) };
        <span class="comment">// Clear the page marks - we are the only GC thread that is accessing this chunk
        </span><span class="kw">unsafe </span>{ <span class="self">self</span>.unset_page_mark(chunk_start, BYTES_IN_CHUNK) };
    }

    <span class="doccomment">/// Sweep an object if it is dead, and unset page marks for empty pages before this object.
    /// Return true if the object is swept.
    </span><span class="kw">fn </span>sweep_object(<span class="kw-2">&amp;</span><span class="self">self</span>, object: ObjectReference, empty_page_start: <span class="kw-2">&amp;mut </span>Address) -&gt; bool {
        <span class="kw">let </span>(obj_start, offset_malloc, bytes) = <span class="self">Self</span>::get_malloc_addr_size(object);

        <span class="comment">// We are the only thread that is dealing with the object. We can use non-atomic methods for the metadata.
        </span><span class="kw">if </span>!<span class="kw">unsafe </span>{ is_marked_unsafe::&lt;VM&gt;(object) } {
            <span class="comment">// Dead object
            </span><span class="macro">trace!</span>(<span class="string">&quot;Object {} has been allocated but not marked&quot;</span>, object);

            <span class="comment">// Free object
            </span><span class="self">self</span>.free_internal(obj_start, bytes, offset_malloc);
            <span class="macro">trace!</span>(<span class="string">&quot;free object {}&quot;</span>, object);
            <span class="kw">unsafe </span>{ unset_alloc_bit_unsafe::&lt;VM&gt;(object) };

            <span class="bool-val">true
        </span>} <span class="kw">else </span>{
            <span class="comment">// Live object that we have marked

            // Unset marks for free pages and update last_object_end
            </span><span class="kw">if </span>!empty_page_start.is_zero() {
                <span class="comment">// unset marks for pages since last object
                </span><span class="kw">let </span>current_page = object
                    .to_object_start::&lt;VM&gt;()
                    .align_down(BYTES_IN_MALLOC_PAGE);
                <span class="kw">if </span>current_page &gt; <span class="kw-2">*</span>empty_page_start {
                    <span class="comment">// we are the only GC thread that is accessing this chunk
                    </span><span class="kw">unsafe </span>{
                        <span class="self">self</span>.unset_page_mark(<span class="kw-2">*</span>empty_page_start, current_page - <span class="kw-2">*</span>empty_page_start)
                    };
                }
            }

            <span class="comment">// Update last_object_end
            </span><span class="kw-2">*</span>empty_page_start = (obj_start + bytes).align_up(BYTES_IN_MALLOC_PAGE);

            <span class="bool-val">false
        </span>}
    }

    <span class="doccomment">/// Used when each chunk is done. Only called in debug build.
    </span><span class="attribute">#[cfg(debug_assertions)]
    </span><span class="kw">fn </span>debug_sweep_chunk_done(<span class="kw-2">&amp;</span><span class="self">self</span>, live_bytes_in_the_chunk: usize) {
        <span class="macro">debug!</span>(
            <span class="string">&quot;Used bytes after releasing: {}&quot;</span>,
            <span class="self">self</span>.active_bytes.load(Ordering::SeqCst)
        );

        <span class="kw">let </span>completed_packets = <span class="self">self</span>.completed_work_packets.fetch_add(<span class="number">1</span>, Ordering::SeqCst) + <span class="number">1</span>;
        <span class="self">self</span>.work_live_bytes
            .fetch_add(live_bytes_in_the_chunk, Ordering::SeqCst);

        <span class="kw">if </span>completed_packets == <span class="self">self</span>.total_work_packets.load(Ordering::Relaxed) {
            <span class="macro">trace!</span>(
                <span class="string">&quot;work_live_bytes = {}, live_bytes = {}, active_bytes = {}&quot;</span>,
                <span class="self">self</span>.work_live_bytes.load(Ordering::Relaxed),
                live_bytes_in_the_chunk,
                <span class="self">self</span>.active_bytes.load(Ordering::Relaxed)
            );
            <span class="macro">debug_assert_eq!</span>(
                <span class="self">self</span>.work_live_bytes.load(Ordering::Relaxed),
                <span class="self">self</span>.active_bytes.load(Ordering::Relaxed)
            );
        }
    }

    <span class="doccomment">/// This function is called when the mark bits sit on the side metadata.
    /// This has been optimized with the use of bulk loading and bulk zeroing of
    /// metadata.
    ///
    /// This function uses non-atomic accesses to side metadata (although these
    /// non-atomic accesses should not have race conditions associated with them)
    /// as well as calls libc functions (`malloc_usable_size()`, `free()`)
    </span><span class="kw">fn </span>sweep_chunk_mark_on_side(<span class="kw-2">&amp;</span><span class="self">self</span>, chunk_start: Address, mark_bit_spec: SideMetadataSpec) {
        <span class="comment">// We can do xor on bulk for mark bits and valid object bits. If the result is zero, that means
        // the objects in it are all alive (both valid object bit and mark bit is set), and we do not
        // need to do anything for the region. Otherwise, we will sweep each single object in the region.
        // Note: Enabling this would result in inaccurate page accounting. We disable this by default, and
        // we will sweep object one by one.
        </span><span class="kw">const </span>BULK_XOR_ON_MARK_BITS: bool = <span class="bool-val">false</span>;

        <span class="kw">if </span>BULK_XOR_ON_MARK_BITS {
            <span class="attribute">#[cfg(debug_assertions)]
            </span><span class="kw">let </span><span class="kw-2">mut </span>live_bytes = <span class="number">0</span>;

            <span class="macro">debug!</span>(<span class="string">&quot;Check active chunk {:?}&quot;</span>, chunk_start);
            <span class="kw">let </span><span class="kw-2">mut </span>address = chunk_start;
            <span class="kw">let </span>chunk_end = chunk_start + BYTES_IN_CHUNK;

            <span class="macro">debug_assert!</span>(
                <span class="kw">crate</span>::util::alloc_bit::ALLOC_SIDE_METADATA_SPEC.log_bytes_in_region
                    == mark_bit_spec.log_bytes_in_region,
                <span class="string">&quot;Alloc-bit and mark-bit metadata have different minimum object sizes!&quot;
            </span>);

            <span class="comment">// For bulk xor&#39;ing 128-bit vectors on architectures with vector instructions
            // Each bit represents an object of LOG_MIN_OBJ_SIZE size
            </span><span class="kw">let </span>bulk_load_size: usize =
                <span class="number">128 </span>* (<span class="number">1 </span>&lt;&lt; <span class="kw">crate</span>::util::alloc_bit::ALLOC_SIDE_METADATA_SPEC.log_bytes_in_region);

            <span class="comment">// The start of a possibly empty page. This will be updated during the sweeping, and always points to the next page of last live objects.
            </span><span class="kw">let </span><span class="kw-2">mut </span>empty_page_start = Address::ZERO;

            <span class="comment">// Scan the chunk by every &#39;bulk_load_size&#39; region.
            </span><span class="kw">while </span>address &lt; chunk_end {
                <span class="kw">let </span>alloc_128: u128 =
                    <span class="kw">unsafe </span>{ load128(<span class="kw-2">&amp;</span><span class="kw">crate</span>::util::alloc_bit::ALLOC_SIDE_METADATA_SPEC, address) };
                <span class="kw">let </span>mark_128: u128 = <span class="kw">unsafe </span>{ load128(<span class="kw-2">&amp;</span>mark_bit_spec, address) };

                <span class="comment">// Check if there are dead objects in the bulk loaded region
                </span><span class="kw">if </span>alloc_128 ^ mark_128 != <span class="number">0 </span>{
                    <span class="kw">let </span>end = address + bulk_load_size;

                    <span class="comment">// We will do non atomic load on the alloc bit, as this is the only thread that access the alloc bit for a chunk.
                    // Linear scan through the bulk load region.
                    </span><span class="kw">let </span>bulk_load_scan = <span class="kw">crate</span>::util::linear_scan::ObjectIterator::&lt;
                        VM,
                        MallocObjectSize&lt;VM&gt;,
                        <span class="bool-val">false</span>,
                    &gt;::new(address, end);
                    <span class="kw">for </span>object <span class="kw">in </span>bulk_load_scan {
                        <span class="self">self</span>.sweep_object(object, <span class="kw-2">&amp;mut </span>empty_page_start);
                    }
                } <span class="kw">else </span>{
                    <span class="comment">// TODO we aren&#39;t actually accounting for the case where an object is alive and spans
                    // a page boundary as we don&#39;t know what the object sizes are/what is alive in the bulk region
                    </span><span class="kw">if </span>alloc_128 != <span class="number">0 </span>{
                        empty_page_start = address + bulk_load_size;
                    }
                }

                <span class="comment">// We have processed this bulk load memory. Step to the next.
                </span>address += bulk_load_size;
                <span class="macro">debug_assert!</span>(address.is_aligned_to(bulk_load_size));
            }

            <span class="comment">// Linear scan through the chunk, and add up all the live object sizes.
            // We have to do this as a separate pass, as in the above pass, we did not go through all the live objects
            </span><span class="attribute">#[cfg(debug_assertions)]
            </span>{
                <span class="kw">let </span>chunk_linear_scan = <span class="kw">crate</span>::util::linear_scan::ObjectIterator::&lt;
                    VM,
                    MallocObjectSize&lt;VM&gt;,
                    <span class="bool-val">false</span>,
                &gt;::new(chunk_start, chunk_end);
                <span class="kw">for </span>object <span class="kw">in </span>chunk_linear_scan {
                    <span class="kw">let </span>(obj_start, <span class="kw">_</span>, bytes) = <span class="self">Self</span>::get_malloc_addr_size(object);

                    <span class="kw">if </span>ASSERT_ALLOCATION {
                        <span class="macro">debug_assert!</span>(
                            <span class="self">self</span>.active_mem.lock().unwrap().contains_key(<span class="kw-2">&amp;</span>obj_start),
                            <span class="string">&quot;Address {} with alloc bit is not in active_mem&quot;</span>,
                            obj_start
                        );
                        <span class="macro">debug_assert_eq!</span>(
                            <span class="self">self</span>.active_mem.lock().unwrap().get(<span class="kw-2">&amp;</span>obj_start),
                            <span class="prelude-val">Some</span>(<span class="kw-2">&amp;</span>bytes),
                            <span class="string">&quot;Address {} size in active_mem does not match the size from malloc_usable_size&quot;</span>,
                            obj_start
                        );
                    }

                    <span class="macro">debug_assert!</span>(
                        <span class="kw">unsafe </span>{ is_marked_unsafe::&lt;VM&gt;(object) },
                        <span class="string">&quot;Dead object = {} found after sweep&quot;</span>,
                        object
                    );

                    live_bytes += bytes;
                }
            }

            <span class="comment">// Clear all the mark bits
            </span>mark_bit_spec.bzero_metadata(chunk_start, BYTES_IN_CHUNK);

            <span class="comment">// If we never updated empty_page_start, the entire chunk is empty.
            </span><span class="kw">if </span>empty_page_start.is_zero() {
                <span class="self">self</span>.clean_up_empty_chunk(chunk_start);
            }

            <span class="attribute">#[cfg(debug_assertions)]
            </span><span class="self">self</span>.debug_sweep_chunk_done(live_bytes);
        } <span class="kw">else </span>{
            <span class="self">self</span>.sweep_each_object_in_chunk(chunk_start);
        }
    }

    <span class="doccomment">/// This sweep function is called when the mark bit sits in the object header
    ///
    /// This function uses non-atomic accesses to side metadata (although these
    /// non-atomic accesses should not have race conditions associated with them)
    /// as well as calls libc functions (`malloc_usable_size()`, `free()`)
    </span><span class="kw">fn </span>sweep_chunk_mark_in_header(<span class="kw-2">&amp;</span><span class="self">self</span>, chunk_start: Address) {
        <span class="self">self</span>.sweep_each_object_in_chunk(chunk_start)
    }

    <span class="kw">fn </span>sweep_each_object_in_chunk(<span class="kw-2">&amp;</span><span class="self">self</span>, chunk_start: Address) {
        <span class="attribute">#[cfg(debug_assertions)]
        </span><span class="kw">let </span><span class="kw-2">mut </span>live_bytes = <span class="number">0</span>;

        <span class="macro">debug!</span>(<span class="string">&quot;Check active chunk {:?}&quot;</span>, chunk_start);

        <span class="comment">// The start of a possibly empty page. This will be updated during the sweeping, and always points to the next page of last live objects.
        </span><span class="kw">let </span><span class="kw-2">mut </span>empty_page_start = Address::ZERO;

        <span class="kw">let </span>chunk_linear_scan = <span class="kw">crate</span>::util::linear_scan::ObjectIterator::&lt;
            VM,
            MallocObjectSize&lt;VM&gt;,
            <span class="bool-val">false</span>,
        &gt;::new(chunk_start, chunk_start + BYTES_IN_CHUNK);

        <span class="kw">for </span>object <span class="kw">in </span>chunk_linear_scan {
            <span class="attribute">#[cfg(debug_assertions)]
            </span><span class="kw">if </span>ASSERT_ALLOCATION {
                <span class="kw">let </span>(obj_start, <span class="kw">_</span>, bytes) = <span class="self">Self</span>::get_malloc_addr_size(object);
                <span class="macro">debug_assert!</span>(
                    <span class="self">self</span>.active_mem.lock().unwrap().contains_key(<span class="kw-2">&amp;</span>obj_start),
                    <span class="string">&quot;Address {} with alloc bit is not in active_mem&quot;</span>,
                    obj_start
                );
                <span class="macro">debug_assert_eq!</span>(
                    <span class="self">self</span>.active_mem.lock().unwrap().get(<span class="kw-2">&amp;</span>obj_start),
                    <span class="prelude-val">Some</span>(<span class="kw-2">&amp;</span>bytes),
                    <span class="string">&quot;Address {} size in active_mem does not match the size from malloc_usable_size&quot;</span>,
                    obj_start
                );
            }

            <span class="kw">let </span>live = !<span class="self">self</span>.sweep_object(object, <span class="kw-2">&amp;mut </span>empty_page_start);
            <span class="kw">if </span>live {
                <span class="comment">// Live object. Unset mark bit.
                // We should be the only thread that access this chunk, it is okay to use non-atomic store.
                </span><span class="kw">unsafe </span>{ unset_mark_bit::&lt;VM&gt;(object) };

                <span class="attribute">#[cfg(debug_assertions)]
                </span>{
                    <span class="comment">// Accumulate live bytes
                    </span><span class="kw">let </span>(<span class="kw">_</span>, <span class="kw">_</span>, bytes) = <span class="self">Self</span>::get_malloc_addr_size(object);
                    live_bytes += bytes;
                }
            }
        }

        <span class="comment">// If we never updated empty_page_start, the entire chunk is empty.
        </span><span class="kw">if </span>empty_page_start.is_zero() {
            <span class="self">self</span>.clean_up_empty_chunk(chunk_start);
        } <span class="kw">else if </span>empty_page_start &lt; chunk_start + BYTES_IN_CHUNK {
            <span class="comment">// This is for the edge case where we have a live object and then no other live
            // objects afterwards till the end of the chunk. For example consider chunk
            // 0x0-0x400000 where only one object at 0x100 is alive. We will unset page bits
            // for 0x0-0x100 but then not unset it for the pages after 0x100. This checks
            // if we have empty pages at the end of a chunk that needs to be cleared.
            </span><span class="kw">unsafe </span>{
                <span class="self">self</span>.unset_page_mark(
                    empty_page_start,
                    chunk_start + BYTES_IN_CHUNK - empty_page_start,
                )
            };
        }

        <span class="attribute">#[cfg(debug_assertions)]
        </span><span class="self">self</span>.debug_sweep_chunk_done(live_bytes);
    }
}

<span class="kw">struct </span>MallocObjectSize&lt;VM&gt;(PhantomData&lt;VM&gt;);
<span class="kw">impl</span>&lt;VM: VMBinding&gt; <span class="kw">crate</span>::util::linear_scan::LinearScanObjectSize <span class="kw">for </span>MallocObjectSize&lt;VM&gt; {
    <span class="kw">fn </span>size(object: ObjectReference) -&gt; usize {
        <span class="kw">let </span>(<span class="kw">_</span>, <span class="kw">_</span>, bytes) = MallocSpace::&lt;VM&gt;::get_malloc_addr_size(object);
        bytes
    }
}

<span class="kw">use </span><span class="kw">crate</span>::scheduler::GCWork;
<span class="kw">use </span><span class="kw">crate</span>::MMTK;

<span class="doccomment">/// Simple work packet that just sweeps a single chunk
</span><span class="kw">pub struct </span>MSSweepChunk&lt;VM: VMBinding&gt; {
    ms: <span class="kw-2">&amp;</span><span class="lifetime">&#39;static </span>MallocSpace&lt;VM&gt;,
    <span class="comment">// starting address of a chunk
    </span>chunk: Address,
}

<span class="kw">impl</span>&lt;VM: VMBinding&gt; GCWork&lt;VM&gt; <span class="kw">for </span>MSSweepChunk&lt;VM&gt; {
    <span class="kw">fn </span>do_work(<span class="kw-2">&amp;mut </span><span class="self">self</span>, _worker: <span class="kw-2">&amp;mut </span>GCWorker&lt;VM&gt;, _mmtk: <span class="kw-2">&amp;</span><span class="lifetime">&#39;static </span>MMTK&lt;VM&gt;) {
        <span class="self">self</span>.ms.sweep_chunk(<span class="self">self</span>.chunk);
    }
}
</code></pre></div>
</section></div></main><div id="rustdoc-vars" data-root-path="../../../../../" data-current-crate="mmtk" data-themes="ayu,dark,light" data-resource-suffix="" data-rustdoc-version="1.66.1 (90743e729 2023-01-10)" ></div></body></html>